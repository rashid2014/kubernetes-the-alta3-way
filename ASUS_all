---
ansible_python_interpreter: "/usr/bin/python3"

# control flow
run_all: True
run_makecerts: "{{ run_all }}" 
run_nodeconfig: "{{ run_all }}" 
run_localconfig: "{{ run_all }}" 
run_services: "{{ run_all }}" 
run_bootstrap: "{{ run_all }}" 
run_etcd: "{{ run_all }}"
run_master: "{{ run_all }}"
run_nginx_lb: "{{ run_all }}"
run_nodes: "{{ run_all }}"
run_rbac: "{{ run_all }}"
run_trusted_ca: "{{ run_all }}"

# versions
k8s_version: "1.9.10"
etcd_version: "3.3.9"

# default, override with host vars
xenial: false


cluster_name: "kubernetes-the-alta3-way"

# cluster-cidr is the range of IP addresses for all pod nets usually called the "Pod Network"
# This is the network, which is used by the pods. Since we are employing CIDR network, using CNI, then
# this a large network called cluster-cidr , divided into subnets corresponding to our worker nodes.
# The routing table of the router handling your part of infrastructure network will need to be updated with routes 
# to these pod subnets. Do not confuse this with the Cluster Service Network which is defined below.
cluster_cidr: "10.48.0.0/12"

# The IPv4 Pool calico creates if none exists at start up. 
calico_ipv4pool_cidr: "{{ cluster_cidr }}"

# dependencies and their destinations
cfssl_url: "https://pkg.cfssl.org/R1.2/cfssl_linux-amd64"
cfssljson_url: "https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64"

# masters, nodes and localhost all need kubectl
kubectl:
  url: "https://storage.googleapis.com/kubernetes-release/release/v{{ k8s_version }}/bin/linux/amd64/kubectl"
  path: "/usr/local/bin/kubectl"

# config and cert dirs
localhost_home_dir: "{{ lookup('env', 'HOME') }}"
home_dir: "{{ ansible_env.HOME }}"
cert_dir: "{{ home_dir }}/k8s-certs"
config_dir: "{{ home_dir }}/k8s-config"

# csr config
csr_dir: "{{ config_dir }}/csr/"
country_csr: US
city_csr: Harrisburg
state_csr: Pennsylvania
ou_csr: Kubernetes The Alta3 Way

# cert config files
ca_config_file: "{{ config_dir }}/ca-config.json"
encryption_config_file: "{{ config_dir }}/encryption-config.yaml"

ca_csr_config_file: "{{ csr_dir }}/ca-csr.json"
k8s_csr_config_file: "{{ csr_dir }}/kubernetes-csr.json"
admin_csr_config_file: "{{ csr_dir }}/admin-csr.json" 
kube_proxy_csr_config_file: "{{ csr_dir }}/kube-proxy-csr.json"
kube_controller_manager_csr_config_file: "{{ csr_dir }}/kube-controller-manager-csr.json"
kube_scheduler_csr_config_file: "{{ csr_dir }}/kube-scheduler-csr.json"
kube_service_account_csr_config_file: "{{ csr_dir }}/kube-service-account-csr.json"
kube_registry_web_csr_config_file: "{{ csr_dir }}/kube-registry-web-csr.json"

# generated cert and key files
ca_pem_file: "{{ cert_dir }}/ca.pem"
ca_crt_file: "{{ cert_dir }}/ca.crt"
ca_key_file: "{{ cert_dir }}/ca-key.pem"
k8s_pem_file: "{{ cert_dir }}/kubernetes.pem"
k8s_key_file: "{{ cert_dir }}/kubernetes-key.pem"
admin_pem_file: "{{ cert_dir }}/admin.pem"
admin_key_file: "{{ cert_dir }}/admin-key.pem"
kube_proxy_pem_file: "{{ cert_dir }}/kube-proxy.pem"
kube_proxy_key_file: "{{ cert_dir }}/kube-proxy-key.pem"
kube_controller_manager_pem_file: "{{ cert_dir }}/kube-controller-manager.pem"
kube_controller_manager_key_file: "{{ cert_dir }}/kube-controller-manager-key.pem"
kube_scheduler_pem_file: "{{ cert_dir }}/kube-scheduler.pem"
kube_scheduler_key_file: "{{ cert_dir }}/kube-scheduler-key.pem"
kube_service_account_pem_file: "{{ cert_dir }}/service-account.pem"
kube_service_account_key_file: "{{ cert_dir }}/service-account-key.pem"
kube_registry_web_pem_file: "{{ cert_dir }}/registry-web.pem"
kube_registry_web_key_file: "{{ cert_dir }}/registry-web-key.pem"

# master and node certificates
deployed_k8s_config_dir: "/var/lib/kubernetes"
deployed_ca_pem_file: "{{ deployed_k8s_config_dir }}/ca.pem"
deployed_ca_key_file: "{{ deployed_k8s_config_dir }}/ca-key.pem"
deployed_k8s_pem_file: "{{ deployed_k8s_config_dir }}/kubernetes.pem"
deployed_k8s_key_file: "{{ deployed_k8s_config_dir }}/kubernetes-key.pem"
deployed_encryption_config_file: "{{ deployed_k8s_config_dir }}/encryption-config.yaml"
deployed_kube_scheduler_pem_file: "{{ deployed_k8s_config_dir }}/kube-scheduler.pem"
deployed_kube_scheduler_key_file: "{{ deployed_k8s_config_dir }}/kube-scheduler-key.pem"

# deployed to both masters and nodes
deployed_cert_and_config_files:
 - src: "{{ ca_pem_file }}"
   dest: "{{ deployed_ca_pem_file }}"
   mode: "0664"
 - src: "{{ ca_key_file }}"
   dest: "{{ deployed_ca_key_file }}"
   mode: "0600" 
 - src: "{{ k8s_pem_file }}"
   dest: "{{ deployed_k8s_pem_file }}"
   mode: "0664" 
 - src: "{{ k8s_key_file }}"
   dest: "{{ deployed_k8s_key_file }}"
   mode: "0600" 
 - src: "{{ encryption_config_file }}"
   dest: "{{ deployed_encryption_config_file }}"
   mode: "0600" 
 - src: "{{ kube_service_account_pem_file }}"
   dest: "{{ deployed_kube_scheduler_pem_file }}"
   mode: "0664"
 - src: "{{ kube_service_account_key_file }}"
   dest: "{{ deployed_kube_scheduler_key_file }}"
   mode: "0600" 

auth_kubeconfigs:
   - name: "kube-proxy"
     server: "{{ localhost_lb_apiserver_address }}"
     config: "kube-proxy.kubeconfig"
     pem: "{{ kube_proxy_pem_file }}"
     key: "{{ kube_proxy_key_file }}"
     user: "kube-proxy"

   - name: "kube-controller-manager"
     server: "{{ master_api_localhost_url }}"
     config: "kube-controller-manager.kubeconfig"
     pem: "{{ kube_controller_manager_pem_file }}"
     key: "{{ kube_controller_manager_key_file }}"
     user: "system:kube-controller-manager"

   - name: "kube-scheduler"
     server: "{{ master_api_localhost_url }}"
     config: "kube-scheduler.kubeconfig"
     pem: "{{ kube_scheduler_pem_file }}"
     key: "{{ kube_scheduler_key_file }}"
     user: "system:kube-scheduler"

   - name: "admin"
     server: "{{ localhost_lb_apiserver_address }}"
     config: "admin.kubeconfig"
     pem: "{{ admin_pem_file }}"
     key: "{{ admin_key_file }}"
     user: "admin"



# MASTER VARS
# The SERVICE NETWORK is a **completely virtual** network, which is used to assign IP addresses to Kubernetes Services,
# which you will be creating. A Service is a frontend to a Deployment. It must be noted that IP from this network are
# never assigned to any of the interfaces of any of the nodes/VMs, etc. These (Service IPs) are used behind the scenes
# by kube-proxy to create iptables rules on the worker nodes.
apiserver_cluster_ip: "10.19.0.1"
service_cluster_ip_range: "10.19.0.0/16"
cluster_dns: "10.19.0.10"

master_ips_csv: "{% for m in groups['masters'] %}{{ hostvars[m].ansible_host }}{% if not loop.last%},{% endif %}{% endfor %}"
master_api_localhost_url: "https://127.0.0.1:6443" 

master_services: 
 - display_name: "Kubernetes API Server"
   name: "kube-apiserver"
   url:  "https://storage.googleapis.com/kubernetes-release/release/v{{ k8s_version }}/bin/linux/amd64/kube-apiserver"
   path: "/usr/local/bin/kube-apiserver"
   mode: "0775"
   service_j2: "{{ role_path }}/templates/kube-apiserver.service.j2"
   service_path: "/etc/systemd/system/kube-apiserver.service"
   
 - display_name: "Kubernetes Manager"
   name: "kube-controller-manager"
   url: "https://storage.googleapis.com/kubernetes-release/release/v{{ k8s_version }}/bin/linux/amd64/kube-controller-manager" 
   path: "/usr/local/bin/kube-controller-manager"
   mode: "0775"
   service_j2: "{{ role_path }}/templates/kube-controller-manager.service.j2"
   service_path: "/etc/systemd/system/kube-controller-manager.service"
   
 - display_name: "Kubernetes Scheduler"
   name: "kube-scheduler"
   url: "https://storage.googleapis.com/kubernetes-release/release/v{{ k8s_version }}/bin/linux/amd64/kube-scheduler"
   path: "/usr/local/bin/kube-scheduler"
   mode: "0775"
   service_j2: "{{ role_path }}/files/kube-scheduler.service"
   service_path: "/etc/systemd/system/kube-scheduler.service"

master_netconfig: |
                    network:
                        version: 2
                        ethernets:
                            # 1 gig ethernet right interface
                            eno1:
                                addresses: []
                                dhcp4: true
                                optional: true
                            # 1 gig ethernet left interface
                            eno2:
                                addresses: []
                                dhcp4: true
                                optional: true
                            # 10 gig ethernet right interface
                            ens2f0:
                                addresses: []
                                dhcp4: true
                                optional: true
                            # 10 gig ethernet left interface
                            ens2f1:
                                addresses: []
                                dhcp4: true
                                nameservers:
                                  addresses: [{{ cluster_dns }}]
                                routes:
                                - to: 10.31.0.0/16   
                                  via: 10.16.2.31
                                - to: 10.32.0.0/16   
                                  via: 10.16.2.32
#                                   "{% for host in {{ groups['nodes'] }} %}"
#                                 - to:  "{{ hostvars[host].cni_host_subnet }}"
#                                   via: "{{ hostvars[host].ansible_host }}"
#                                   "{% endfor %}"
                    


node_services: 
 - name: "kubelet"
   url: "https://storage.googleapis.com/kubernetes-release/release/v{{ k8s_version}}/bin/linux/amd64/kube-proxy"
   path: "/usr/local/bin/kube-proxy"
   mode: "0775"
   service_j2: "{{ role_path }}/templates/kube-proxy.service.j2"
   service_path: "/etc/systemd/system/kube-proxy.service" 

 - name: "kube-proxy"
   url: "https://storage.googleapis.com/kubernetes-release/release/v{{k8s_version}}/bin/linux/amd64/kubelet"
   path: "/usr/local/bin/kubelet"
   mode: "0775"
   service_j2: "{{ role_path }}/templates/kubelet.service.j2"
   service_path: "/etc/systemd/system/kubelet.service" 

node_dependencies:
 - name: "containerd"
   url: "https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz"
   dest: "/opt/cni/bin/"
 - name: "cri-containerd"
   url: "https://github.com/containerd/cri-containerd/releases/download/v1.0.0-beta.1/cri-containerd-1.0.0-beta.1.linux-amd64.tar.gz"
   dest: "/" 


# rbac configs
master_rbac_role:
  file: "{{ role_path }}/files/rbac-clusterrole.yaml"
  path: "{{ config_dir }}/rbac-clusterrole.yaml"

master_rbac_rolebind:
  file: "{{ role_path }}/files/rbac-clusterrolebind.yaml"
  path: "{{ config_dir }}/rbac-clusterrolebind.yaml"
 
# etcd
etcd_arch: "amd64"
etcd_file_name: "etcd-v{{ etcd_version }}-linux-{{ etcd_arch }}"
etcd_tar_name: "{{ etcd_file_name }}.tar.gz"
etcd_url: "https://github.com/coreos/etcd/releases/download/v{{ etcd_version }}/{{ etcd_tar_name }}"
etcd_extracted_dir: "/tmp/{{ etcd_file_name }}"
etcd_port:
  client: 2379
  s2s: 2380 
etcd_cluster_urls: "{% for e in groups['etcd'] %}{{ hostvars[e].inventory_hostname }}=https://{{ hostvars[e].ansible_host }}:{{ etcd_port.s2s }}{% if not loop.last%},{% endif %}{% endfor %}"
master_etcd_cluster_urls: "{% for e in groups['etcd'] %}https://{{ hostvars[e].ansible_host }}:{{ etcd_port.client }}{% if not loop.last%},{% endif %}{% endfor %}"
calico_etcd_endpoints: "{{ master_etcd_cluster_urls }}"

# nodes 
localhost_lb_apiserver_address: "https://127.0.0.1:6443"
kubelet_kubeconfig_path: "/var/lib/kubelet/kubeconfig"
kube_proxy_kubeconfig_path: "/var/lib/kube-proxy/kubeconfig"
